{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Journey to Springfield"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!  Training on GPU ...\n",
      "Mon Sep 18 09:23:04 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 536.67                 Driver Version: 536.67       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 2060      WDDM  | 00000000:07:00.0  On |                  N/A |\n",
      "|  0%   44C    P8              18W / 170W |   2071MiB /  6144MiB |     13%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      3224    C+G   ...GeForce Experience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A      7376    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A      8816    C+G   ...siveControlPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A      9000    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A      9020    C+G   ...64__8wekyb3d8bbwe\\CalculatorApp.exe    N/A      |\n",
      "|    0   N/A  N/A      9964    C+G   ...aming\\Telegram Desktop\\Telegram.exe    N/A      |\n",
      "|    0   N/A  N/A     10068    C+G   ...2txyewy\\StartMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     11856    C+G   ...1.0_x64__8wekyb3d8bbwe\\Video.UI.exe    N/A      |\n",
      "|    0   N/A  N/A     12928    C+G   ...oogle\\Chrome\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     14492    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     14548    C+G   ...GeForce Experience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     14756    C+G   C:\\Windows\\explorer.exe                   N/A      |\n",
      "|    0   N/A  N/A     18076    C+G   ...s\\VSCode\\Microsoft VS Code\\Code.exe    N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from os.path import exists\n",
    "import PIL\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')\n",
    "    !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "from torchvision import transforms\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "from matplotlib import colors, pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# в sklearn не все гладко, чтобы в colab удобно выводить картинки\n",
    "# мы будем игнорировать warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# разные режимы датасета\n",
    "DATA_MODES = ['train', 'val', 'test']\n",
    "# все изображения будут масштабированы к размеру 224x224 px\n",
    "RESCALE_SIZE = 224\n",
    "# работаем на видеокарте\n",
    "DEVICE = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = Path('data/train/simpsons_dataset')\n",
    "TEST_DIR = Path('data/testset/testset')\n",
    "\n",
    "train_val_files = sorted(list(TRAIN_DIR.rglob('*.jpg')))\n",
    "test_files = sorted(list(TEST_DIR.rglob('*.jpg')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_val_labels = [path.parent.name for path in train_val_files]\n",
    "train_files, val_files = train_test_split(train_val_files, test_size=0.25, \\\n",
    "                                          stratify=train_val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for training and evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.inception import InceptionOutputs\n",
    "\n",
    "def fit_epoch(model, train_loader, criterion, optimizer):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    processed_data = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        #<1>\n",
    "        if isinstance(outputs, InceptionOutputs):\n",
    "            outputs = torch.nn.functional.softmax(outputs[0], dim=0)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        preds = torch.argmax(outputs, 1)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        processed_data += inputs.size(0)\n",
    "\n",
    "    train_loss = running_loss / processed_data\n",
    "    train_acc = running_corrects.cpu().numpy() / processed_data\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def eval_epoch(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    processed_size = 0\n",
    "\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            preds = torch.argmax(outputs, 1)\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        f1 = f1_score(labels.data.to('cpu'), preds.to('cpu'), average='micro')\n",
    "        processed_size += inputs.size(0)\n",
    "    val_loss = running_loss / processed_size\n",
    "    val_acc = running_corrects.double() / processed_size\n",
    "    return val_loss, val_acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "\n",
    "def train(train_dl, val_dl, model, epochs, optimizer, batch_size=64):\n",
    "    best_f1 = 0\n",
    "    #save_path = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M\"))\n",
    "\n",
    "    train_loader = train_dl #DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = val_dl #DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    history = []\n",
    "    log_template = \"Epoch {ep:03d} train_loss: {t_loss:0.4f} \\\n",
    "    val_loss {v_loss:0.4f}\\ntrain_acc {t_acc:0.4f} val_acc {v_acc:0.4f}\\n{time}\\nF1score {f1}\\n\"\n",
    "\n",
    "    print('Training started at', datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:\n",
    "        opt = optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            train_loss, train_acc = fit_epoch(model, train_loader, criterion, opt)\n",
    "            print(\"loss\", train_loss)\n",
    "\n",
    "            val_loss, val_acc, f1 = eval_epoch(model, val_loader, criterion)\n",
    "            history.append((train_loss, train_acc, val_loss, val_acc))\n",
    "\n",
    "            if f1 > best_f1:\n",
    "                save_path = 'model_f1_{}_{}.pth'.format(f1, datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M\"))\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "                best_f1 = f1\n",
    "            \n",
    "            \n",
    "\n",
    "            pbar_outer.update(1)\n",
    "            tqdm.write(log_template.format(ep=epoch+1, t_loss=train_loss,\\\n",
    "                                           v_loss=val_loss, t_acc=train_acc, v_acc=val_acc,\n",
    "                                           time=datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                                           f1=f1))\n",
    "            \n",
    "        print('Training end at', datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_loader):\n",
    "    with torch.no_grad():\n",
    "        logits = []\n",
    "\n",
    "        for inputs in test_loader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            model.eval()\n",
    "            outputs = model(inputs.cpu()).cpu()\n",
    "            logits.append(outputs)\n",
    "\n",
    "    probs = nn.functional.softmax(torch.cat(logits), dim=-1).numpy()\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_loss(history):\n",
    "    loss, acc, val_loss, val_acc = zip(*history)\n",
    "    plt.figure(figsize=(15, 9))\n",
    "    plt.plot(loss, label=\"train_loss\")\n",
    "    plt.plot(val_loss, label=\"val_loss\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(np.unique(train_val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimpsonsDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpsonsDs(Dataset):\n",
    "    \"\"\"\n",
    "    Датасет с картинками, который паралельно подгружает их из папок\n",
    "    производит скалирование и превращение в торчевые тензоры\n",
    "    \"\"\"\n",
    "    def __init__(self, files, mode, transform=None, image_size=(224, 224)):\n",
    "        super().__init__()\n",
    "        # список файлов для загрузки\n",
    "        self.files = sorted(files)\n",
    "        # режим работы\n",
    "        self.mode = mode\n",
    "\n",
    "        self.image_size = image_size\n",
    "\n",
    "        if self.mode not in DATA_MODES:\n",
    "            print(f\"{self.mode} is not correct; correct modes: {DATA_MODES}\")\n",
    "            raise NameError\n",
    "\n",
    "        self.len_ = len(self.files)\n",
    "\n",
    "        self.label_encoder = LabelEncoder()\n",
    "\n",
    "        if (transform is None): # or (self.mode =='test') or (self.mode =='val') :\n",
    "            transform = transforms.Compose([\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225],), \n",
    "                            transforms.Resize(self.image_size, antialias=True)\n",
    "                        ])\n",
    "        \n",
    "        self.transform = transform\n",
    "\n",
    "        if self.mode != 'test':\n",
    "            self.labels = [path.parent.name for path in self.files]\n",
    "            self.label_encoder.fit(self.labels)\n",
    "\n",
    "            with open('label_encoder.pkl', 'wb') as le_dump_file:\n",
    "                  pickle.dump(self.label_encoder, le_dump_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len_\n",
    "\n",
    "    def load_sample(self, file):\n",
    "        image = Image.open(file)\n",
    "        image.load()\n",
    "        return image\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # для преобразования изображений в тензоры PyTorch и нормализации входа\n",
    "        \n",
    "        x = self.load_sample(self.files[index])\n",
    "        #x = self._prepare_sample(x)\n",
    "        #x = np.array(x / 255, dtype='float32')\n",
    "        x = self.transform(x)\n",
    "        if self.mode == 'test':\n",
    "            return x\n",
    "        else:\n",
    "            label = self.labels[index]\n",
    "            label_id = self.label_encoder.transform([label])\n",
    "            y = label_id.item()\n",
    "            return x, y\n",
    "\n",
    "    def _prepare_sample(self, image):\n",
    "        image = image.resize((RESCALE_SIZE, RESCALE_SIZE))\n",
    "        return np.array(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpsonsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Датасет с картинками, который паралельно подгружает их из папок\n",
    "    производит скалирование и превращение в торчевые тензоры\n",
    "    \"\"\"\n",
    "    def __init__(self, files, mode):\n",
    "        super().__init__()\n",
    "        # список файлов для загрузки\n",
    "        self.files = sorted(files)\n",
    "        # режим работы\n",
    "        self.mode = mode\n",
    "\n",
    "        if self.mode not in DATA_MODES:\n",
    "            print(f\"{self.mode} is not correct; correct modes: {DATA_MODES}\")\n",
    "            raise NameError\n",
    "\n",
    "        self.len_ = len(self.files)\n",
    "\n",
    "        self.label_encoder = LabelEncoder()\n",
    "\n",
    "        if self.mode != 'test':\n",
    "            self.labels = [path.parent.name for path in self.files]\n",
    "            self.label_encoder.fit(self.labels)\n",
    "\n",
    "            with open('label_encoder.pkl', 'wb') as le_dump_file:\n",
    "                  pickle.dump(self.label_encoder, le_dump_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len_\n",
    "\n",
    "    def load_sample(self, file):\n",
    "        image = Image.open(file)\n",
    "        image.load()\n",
    "        return image\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # для преобразования изображений в тензоры PyTorch и нормализации входа\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225],\n",
    "                                 )\n",
    "        ])\n",
    "        x = self.load_sample(self.files[index])\n",
    "        x = self._prepare_sample(x)\n",
    "        x = np.array(x / 255, dtype='float32')\n",
    "        x = transform(x)\n",
    "        if self.mode == 'test':\n",
    "            return x\n",
    "        else:\n",
    "            label = self.labels[index]\n",
    "            label_id = self.label_encoder.transform([label])\n",
    "            y = label_id.item()\n",
    "            return x, y\n",
    "\n",
    "    def _prepare_sample(self, image):\n",
    "        image = image.resize((RESCALE_SIZE, RESCALE_SIZE))\n",
    "        return np.array(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for calc accuracy per character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'groundskeeper_willie': [0, 0], 'edna_krabappel': [0, 0], 'agnes_skinner': [0, 0], 'homer_simpson': [0, 0], 'maggie_simpson': [0, 0], 'selma_bouvier': [0, 0], 'otto_mann': [0, 0], 'krusty_the_clown': [0, 0], 'apu_nahasapeemapetilon': [0, 0], 'professor_john_frink': [0, 0], 'troy_mcclure': [0, 0], 'miss_hoover': [0, 0], 'barney_gumble': [0, 0], 'lisa_simpson': [0, 0], 'fat_tony': [0, 0], 'abraham_grampa_simpson': [0, 0], 'marge_simpson': [0, 0], 'comic_book_guy': [0, 0], 'snake_jailbird': [0, 0], 'ned_flanders': [0, 0], 'patty_bouvier': [0, 0], 'sideshow_mel': [0, 0], 'principal_skinner': [0, 0], 'cletus_spuckler': [0, 0], 'waylon_smithers': [0, 0], 'milhouse_van_houten': [0, 0], 'carl_carlson': [0, 0], 'ralph_wiggum': [0, 0], 'martin_prince': [0, 0], 'lenny_leonard': [0, 0], 'sideshow_bob': [0, 0], 'charles_montgomery_burns': [0, 0], 'disco_stu': [0, 0], 'moe_szyslak': [0, 0], 'chief_wiggum': [0, 0], 'nelson_muntz': [0, 0], 'mayor_quimby': [0, 0], 'bart_simpson': [0, 0], 'kent_brockman': [0, 0], 'gil': [0, 0], 'rainier_wolfcastle': [0, 0]}\n"
     ]
    }
   ],
   "source": [
    "uniqe_labels = list(set(train_val_labels))\n",
    "simpsons_tp_all = {}\n",
    "for i in uniqe_labels:\n",
    "    simpsons_tp_all[i] = [0,0]\n",
    "print(simpsons_tp_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prescision_per_class(model, dataset, batch_size=128):\n",
    "    simpsons_tp_all = {}\n",
    "    for i in uniqe_labels:\n",
    "        simpsons_tp_all[i] = [0,0]\n",
    "\n",
    "    simpl_dl = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in simpl_dl:\n",
    "            \n",
    "            pred = torch.argmax(model(x), 1)\n",
    "            for i_pr, i_true in zip(pred, y):\n",
    "                current_label = dataset.label_encoder.inverse_transform([i_true])[0]\n",
    "                \n",
    "                simpsons_tp_all[current_label][0] += (i_pr==i_true)\n",
    "                simpsons_tp_all[current_label][1] += 1\n",
    "    return simpsons_tp_all\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_per_character(simpsons_acc_val):\n",
    "    total = 0\n",
    "    pre_table = []\n",
    "    for name, presc in simpsons_acc_val.items():\n",
    "        acc = 0\n",
    "        try:\n",
    "            acc = presc[0]/presc[1]\n",
    "        except:\n",
    "            pass\n",
    "        pre_table.append([name, acc, presc[0], presc[1]])\n",
    "        '''name += ' '*10\n",
    "        presc[0] = int(presc[0])\n",
    "        print(f'{name[0:10]} Acc: {presc[0]/presc[1]:0.4f} {presc[1]}')# {presc[0]:0.4f} / {presc[1]:0.4f}')\n",
    "        total += presc[0]/presc[1]'''\n",
    "    #print(total/len(simpsons_acc_val))\n",
    "    df = pd.DataFrame(pre_table, columns=['Name', 'Acc', 'TP', 'Amount'])\n",
    "    print(df.sort_values(by='Acc', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пора заняться дисбалансом  \n",
    "код по устранению диспаланса через oversampling был взят у https://stepik.org/users/247846  \n",
    "\n",
    "Идея простая и элегантная, вместо того, чтобы заниматься устранением дисбаланса через WeightedRandomSampler  \n",
    "Мы просто дублируе путь к картинкам персонажей, у которых количество экземпляров изначально меньше 100.(И мы просто несколько раз обращаемся к одним и тем же картинка при обучении)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = [path.parent.name for path in train_files] # классы train\n",
    "val_labels = [path.parent.name for path in val_files]     # классы val\n",
    "\n",
    "def create_dct_path_labels(train_files, train_labels):\n",
    "    dct_simpsons = {}\n",
    "    for label_i in np.unique(train_labels).tolist():\n",
    "        dct_simpsons[label_i] = []\n",
    "\n",
    "    for path_i, label_i in zip(train_files, train_labels):\n",
    "        dct_simpsons[label_i].append(path_i)\n",
    "\n",
    "    return dct_simpsons\n",
    "\n",
    "def print_dct(dct_simpsons):\n",
    "    for key in dct_simpsons:\n",
    "        print(f\"{key}\\t{dct_simpsons[key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct_path_train = create_dct_path_labels(train_files, train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "increase_to =  300#<4>\n",
    "for person in dct_path_train:\n",
    "    if len(dct_path_train[person]) < 100:\n",
    "        dct_path_train[person] = dct_path_train[person] * (increase_to // len(dct_path_train[person]))\n",
    "        dct_path_train[person].extend(dct_path_train[person][:increase_to - len(dct_path_train[person])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abraham_grampa_simpson\t685\n",
      "agnes_skinner\t300\n",
      "apu_nahasapeemapetilon\t467\n",
      "barney_gumble\t300\n",
      "bart_simpson\t1006\n",
      "carl_carlson\t300\n",
      "charles_montgomery_burns\t895\n",
      "chief_wiggum\t739\n",
      "cletus_spuckler\t300\n",
      "comic_book_guy\t352\n",
      "disco_stu\t300\n",
      "edna_krabappel\t343\n",
      "fat_tony\t300\n",
      "gil\t300\n",
      "groundskeeper_willie\t300\n",
      "homer_simpson\t1684\n",
      "kent_brockman\t373\n",
      "krusty_the_clown\t904\n",
      "lenny_leonard\t233\n",
      "lisa_simpson\t1015\n",
      "maggie_simpson\t300\n",
      "marge_simpson\t968\n",
      "martin_prince\t300\n",
      "mayor_quimby\t185\n",
      "milhouse_van_houten\t809\n",
      "miss_hoover\t300\n",
      "moe_szyslak\t1089\n",
      "ned_flanders\t1090\n",
      "nelson_muntz\t269\n",
      "otto_mann\t300\n",
      "patty_bouvier\t300\n",
      "principal_skinner\t895\n",
      "professor_john_frink\t300\n",
      "rainier_wolfcastle\t300\n",
      "ralph_wiggum\t300\n",
      "selma_bouvier\t300\n",
      "sideshow_bob\t658\n",
      "sideshow_mel\t300\n",
      "snake_jailbird\t300\n",
      "troy_mcclure\t300\n",
      "waylon_smithers\t136\n"
     ]
    }
   ],
   "source": [
    "# Проверим что получилось \n",
    "for person in dct_path_train:\n",
    "    print(f\"{person}\\t{len(dct_path_train[person])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_files = []\n",
    "\n",
    "for person in dct_path_train:\n",
    "    new_train_files.extend(dct_path_train[person])\n",
    "\n",
    "new_train_label = [path.parent.name for path in new_train_files] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Конец скопированного кода"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Запуск модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main model code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "image_size = (224, 224) #<2>\n",
    "scale_grade = 1.5\n",
    "image_size_mod = (int(image_size[0]*scale_grade),\n",
    "                  int(image_size[1]*scale_grade))\n",
    "\n",
    "\n",
    "transform_basic = transforms.Compose([\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225],), \n",
    "                            transforms.Resize(image_size, antialias=True)\n",
    "                        ])\n",
    "\n",
    "transform_main = transforms.Compose([\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225],), \n",
    "                            transforms.Resize(image_size, antialias=True),\n",
    "                            transforms.RandomHorizontalFlip(p=0.5),\n",
    "                            #transforms.RandomVerticalFlip(p=0.3),\n",
    "                            transforms.RandomRotation(degrees=(-45, 45))\n",
    "                        ])\n",
    "\n",
    "transform_pyramid_center = transforms.Compose([\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225],), \n",
    "                            transforms.Resize(image_size_mod, antialias=True),\n",
    "                            transforms.CenterCrop(image_size),\n",
    "                            transforms.RandomHorizontalFlip(p=0.5),\n",
    "                            #transforms.RandomVerticalFlip(p=0.3),\n",
    "                            transforms.RandomRotation(degrees=(-45, 45))\n",
    "                        ])\n",
    "\n",
    "transform_pyramid_rand = transforms.Compose([\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225],), \n",
    "                            transforms.Resize(image_size_mod, antialias=True),\n",
    "                            transforms.RandomCrop(image_size),\n",
    "                            transforms.RandomHorizontalFlip(p=0.5),\n",
    "                            #transforms.RandomVerticalFlip(p=0.3),\n",
    "                            transforms.RandomRotation(degrees=(-45, 45))\n",
    "                        ])\n",
    "\n",
    "val_ds = SimpsonsDs(val_files, mode='val')#, transform=transform)\n",
    "\n",
    " \n",
    "train_ds_basic = SimpsonsDs(new_train_files, mode='train', transform=transform_basic)\n",
    "train_ds_main = SimpsonsDs(new_train_files, mode='train', transform=transform_main)\n",
    "train_ds_center = SimpsonsDs(new_train_files, mode='train', transform=transform_pyramid_center)\n",
    "train_ds_rand = SimpsonsDs(new_train_files, mode='train', transform=transform_pyramid_rand)\n",
    "\n",
    "full_train_dataset = ConcatDataset([train_ds_basic, train_ds_main, train_ds_center, train_ds_rand])\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "big_train_loader = DataLoader(full_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "resnet34_horiz = models.resnet34(pretrained=True)\n",
    "\n",
    "resnet34_horiz.fc = nn.Linear(512, n_classes)\n",
    "# Выключаем подсчет градиентов для слоев, которые не будем обучать\n",
    "for param in resnet34_horiz.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "#<4>\n",
    "\n",
    "for param in resnet34_horiz.layer3.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in resnet34_horiz.layer4.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in resnet34_horiz.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "resnet34_horiz.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.AdamW(resnet34_horiz.parameters(), lr=3e-4)\n",
    "\n",
    "history = train(big_train_loader, val_loader, model=resnet34_horiz, optimizer=optimizer, epochs=7)#, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Эксперемент с разморозокй всех слоев с 1-4 + fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started at 2023-09-18 09:50:03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.32355257072222865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  14%|█▍        | 1/7 [23:40<2:22:01, 1420.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 train_loss: 0.3236     val_loss 0.1856\n",
      "train_acc 0.9181 val_acc 0.9522\n",
      "2023-09-18 10:13:43\n",
      "F1score 0.911504424778761\n",
      "\n",
      "loss 0.11261696286443966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  29%|██▊       | 2/7 [45:03<1:51:37, 1339.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 002 train_loss: 0.1126     val_loss 0.1913\n",
      "train_acc 0.9692 val_acc 0.9541\n",
      "2023-09-18 10:35:06\n",
      "F1score 0.9292035398230089\n",
      "\n",
      "loss 0.08148652308439801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  43%|████▎     | 3/7 [1:06:06<1:26:59, 1304.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 003 train_loss: 0.0815     val_loss 0.1838\n",
      "train_acc 0.9772 val_acc 0.9572\n",
      "2023-09-18 10:56:10\n",
      "F1score 0.911504424778761\n",
      "\n",
      "loss 0.07274445534687571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  57%|█████▋    | 4/7 [1:26:42<1:03:52, 1277.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 004 train_loss: 0.0727     val_loss 0.1688\n",
      "train_acc 0.9800 val_acc 0.9616\n",
      "2023-09-18 11:16:46\n",
      "F1score 0.9026548672566371\n",
      "\n",
      "loss 0.059544699152348904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  71%|███████▏  | 5/7 [1:48:46<43:08, 1294.33s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 005 train_loss: 0.0595     val_loss 0.1787\n",
      "train_acc 0.9835 val_acc 0.9618\n",
      "2023-09-18 11:38:50\n",
      "F1score 0.9469026548672567\n",
      "\n",
      "loss 0.05782810868115401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  86%|████████▌ | 6/7 [2:09:22<21:14, 1274.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 006 train_loss: 0.0578     val_loss 0.1772\n",
      "train_acc 0.9838 val_acc 0.9597\n",
      "2023-09-18 11:59:26\n",
      "F1score 0.9292035398230089\n",
      "\n",
      "loss 0.04947265785896891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 7/7 [2:29:59<00:00, 1285.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 007 train_loss: 0.0495     val_loss 0.1649\n",
      "train_acc 0.9859 val_acc 0.9664\n",
      "2023-09-18 12:20:03\n",
      "F1score 0.9203539823008849\n",
      "\n",
      "Training end at 2023-09-18 12:20:03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "image_size = (224, 224) #<2>\n",
    "scale_grade = 1.5\n",
    "image_size_mod = (int(image_size[0]*scale_grade),\n",
    "                  int(image_size[1]*scale_grade))\n",
    "\n",
    "\n",
    "transform_basic = transforms.Compose([\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225],), \n",
    "                            transforms.Resize(image_size, antialias=True)\n",
    "                        ])\n",
    "\n",
    "transform_main = transforms.Compose([\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225],), \n",
    "                            transforms.Resize(image_size, antialias=True),\n",
    "                            transforms.RandomHorizontalFlip(p=0.5),\n",
    "                            #transforms.RandomVerticalFlip(p=0.3),\n",
    "                            transforms.RandomRotation(degrees=(-45, 45))\n",
    "                        ])\n",
    "\n",
    "transform_pyramid_center = transforms.Compose([\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225],), \n",
    "                            transforms.Resize(image_size_mod, antialias=True),\n",
    "                            transforms.CenterCrop(image_size),\n",
    "                            transforms.RandomHorizontalFlip(p=0.5),\n",
    "                            #transforms.RandomVerticalFlip(p=0.3),\n",
    "                            transforms.RandomRotation(degrees=(-45, 45))\n",
    "                        ])\n",
    "\n",
    "transform_pyramid_rand = transforms.Compose([\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225],), \n",
    "                            transforms.Resize(image_size_mod, antialias=True),\n",
    "                            transforms.RandomCrop(image_size),\n",
    "                            transforms.RandomHorizontalFlip(p=0.5),\n",
    "                            #transforms.RandomVerticalFlip(p=0.3),\n",
    "                            transforms.RandomRotation(degrees=(-45, 45))\n",
    "                        ])\n",
    "\n",
    "val_ds = SimpsonsDs(val_files, mode='val')#, transform=transform)\n",
    "\n",
    " \n",
    "train_ds_basic = SimpsonsDs(new_train_files, mode='train', transform=transform_basic)\n",
    "train_ds_main = SimpsonsDs(new_train_files, mode='train', transform=transform_main)\n",
    "train_ds_center = SimpsonsDs(new_train_files, mode='train', transform=transform_pyramid_center)\n",
    "train_ds_rand = SimpsonsDs(new_train_files, mode='train', transform=transform_pyramid_rand)\n",
    "\n",
    "full_train_dataset = ConcatDataset([train_ds_basic, train_ds_main, train_ds_center, train_ds_rand])\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "big_train_loader = DataLoader(full_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "resnet34_horiz = models.resnet34(pretrained=True)\n",
    "\n",
    "resnet34_horiz.fc = nn.Linear(512, n_classes)\n",
    "# Выключаем подсчет градиентов для слоев, которые не будем обучать\n",
    "for param in resnet34_horiz.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "#<4>\n",
    "for param in resnet34_horiz.layer1.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in resnet34_horiz.layer2.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in resnet34_horiz.layer3.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in resnet34_horiz.layer4.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in resnet34_horiz.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "resnet34_horiz.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.AdamW(resnet34_horiz.parameters(), lr=3e-4)\n",
    "\n",
    "history = train(big_train_loader, val_loader, model=resnet34_horiz, optimizer=optimizer, epochs=7)#, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading model and calc final prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=42, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_model = 'model_f1_0.9557522123893806_2023_09_17_17_26.pth'\n",
    "resnet34_horiz = models.resnet34()\n",
    "resnet34_horiz.fc = nn.Linear(512, 42) # тут должно стоять n_classes но я накосячил и стоит 42\n",
    "resnet34_horiz.load_state_dict(torch.load(path_to_model))\n",
    "resnet34_horiz.eval() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Точность каждого класса на val выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment if error in the cell below\n",
    "#val_ds = SimpsonsDs(val_files, mode='val')#, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Name             Acc           TP  Amount\n",
      "40        rainier_wolfcastle      tensor(1.)   tensor(11)      11\n",
      "11               miss_hoover      tensor(1.)    tensor(4)       4\n",
      "26              carl_carlson      tensor(1.)   tensor(24)      24\n",
      "25       milhouse_van_houten      tensor(1.)  tensor(270)     270\n",
      "24           waylon_smithers      tensor(1.)   tensor(45)      45\n",
      "23           cletus_spuckler      tensor(1.)   tensor(12)      12\n",
      "21              sideshow_mel      tensor(1.)   tensor(10)      10\n",
      "18            snake_jailbird      tensor(1.)   tensor(14)      14\n",
      "14                  fat_tony      tensor(1.)    tensor(7)       7\n",
      "12             barney_gumble      tensor(1.)   tensor(26)      26\n",
      "34              chief_wiggum      tensor(1.)  tensor(247)     247\n",
      "10              troy_mcclure      tensor(1.)    tensor(2)       2\n",
      "5              selma_bouvier      tensor(1.)   tensor(26)      26\n",
      "8     apu_nahasapeemapetilon      tensor(1.)  tensor(156)     156\n",
      "2              agnes_skinner      tensor(1.)   tensor(10)      10\n",
      "38             kent_brockman      tensor(1.)  tensor(125)     125\n",
      "6                  otto_mann      tensor(1.)    tensor(8)       8\n",
      "19              ned_flanders  tensor(0.9973)  tensor(363)     364\n",
      "33               moe_szyslak  tensor(0.9972)  tensor(362)     363\n",
      "13              lisa_simpson  tensor(0.9971)  tensor(338)     339\n",
      "30              sideshow_bob  tensor(0.9954)  tensor(218)     219\n",
      "3              homer_simpson  tensor(0.9947)  tensor(559)     562\n",
      "22         principal_skinner  tensor(0.9933)  tensor(297)     299\n",
      "1             edna_krabappel  tensor(0.9912)  tensor(113)     114\n",
      "15    abraham_grampa_simpson  tensor(0.9912)  tensor(226)     228\n",
      "16             marge_simpson  tensor(0.9907)  tensor(320)     323\n",
      "37              bart_simpson  tensor(0.9881)  tensor(332)     336\n",
      "29             lenny_leonard  tensor(0.9870)   tensor(76)      77\n",
      "7           krusty_the_clown  tensor(0.9868)  tensor(298)     302\n",
      "31  charles_montgomery_burns  tensor(0.9832)  tensor(293)     298\n",
      "17            comic_book_guy  tensor(0.9658)  tensor(113)     117\n",
      "27              ralph_wiggum  tensor(0.9545)   tensor(21)      22\n",
      "36              mayor_quimby  tensor(0.9508)   tensor(58)      61\n",
      "20             patty_bouvier  tensor(0.9444)   tensor(17)      18\n",
      "28             martin_prince  tensor(0.9444)   tensor(17)      18\n",
      "9       professor_john_frink  tensor(0.9375)   tensor(15)      16\n",
      "0       groundskeeper_willie  tensor(0.9333)   tensor(28)      30\n",
      "35              nelson_muntz  tensor(0.9326)   tensor(83)      89\n",
      "4             maggie_simpson  tensor(0.9062)   tensor(29)      32\n",
      "39                       gil  tensor(0.8571)    tensor(6)       7\n",
      "32                 disco_stu  tensor(0.5000)    tensor(1)       2\n"
     ]
    }
   ],
   "source": [
    "simpsons_acc_val = prescision_per_class(resnet34_horiz.to('cpu'), val_ds)\n",
    "acc_per_character(simpsons_acc_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict and save csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = pickle.load(open(\"label_encoder.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = SimpsonsDataset(test_files, mode=\"test\")\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=128)\n",
    "probs = predict(resnet34_horiz, test_loader)\n",
    "\n",
    "\n",
    "preds = label_encoder.inverse_transform(np.argmax(probs, axis=1))\n",
    "test_filenames = [path.name for path in test_dataset.files]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Id                Expected\n",
      "0    img0.jpg            nelson_muntz\n",
      "1    img1.jpg            bart_simpson\n",
      "2   img10.jpg            ned_flanders\n",
      "3  img100.jpg            chief_wiggum\n",
      "4  img101.jpg  apu_nahasapeemapetilon\n",
      "5  img102.jpg           kent_brockman\n",
      "6  img103.jpg          edna_krabappel\n",
      "7  img104.jpg            chief_wiggum\n",
      "8  img105.jpg            lisa_simpson\n",
      "9  img106.jpg           kent_brockman\n"
     ]
    }
   ],
   "source": [
    "my_submitt = pd.DataFrame({'Id':test_filenames, 'Expected':preds})\n",
    "print(my_submitt.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_submitt.to_csv('mysub_baseline.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
